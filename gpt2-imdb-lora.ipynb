{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install -U datasets transformers accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-15T09:33:54.318319Z","iopub.execute_input":"2024-05-15T09:33:54.318992Z","iopub.status.idle":"2024-05-15T09:34:08.086805Z","shell.execute_reply.started":"2024-05-15T09:33:54.318952Z","shell.execute_reply":"2024-05-15T09:34:08.085552Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-05-15T09:37:02.050906Z","iopub.execute_input":"2024-05-15T09:37:02.051690Z","iopub.status.idle":"2024-05-15T09:37:16.118541Z","shell.execute_reply.started":"2024-05-15T09:37:02.051659Z","shell.execute_reply":"2024-05-15T09:37:16.117417Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.40.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.10.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_ckpt = \"openai-community/gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n\nmodel.config.pad_token_id = tokenizer.eos_token_id\ntokenizer.pad_token_id = tokenizer.eos_token_id  ","metadata":{"execution":{"iopub.status.busy":"2024-05-15T11:22:11.347269Z","iopub.execute_input":"2024-05-15T11:22:11.347666Z","iopub.status.idle":"2024-05-15T11:22:11.777564Z","shell.execute_reply.started":"2024-05-15T11:22:11.347637Z","shell.execute_reply":"2024-05-15T11:22:11.776741Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# for name, module in model.named_modules():\n#     print(name)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T11:21:28.066703Z","iopub.execute_input":"2024-05-15T11:21:28.067563Z","iopub.status.idle":"2024-05-15T11:21:28.071291Z","shell.execute_reply.started":"2024-05-15T11:21:28.067531Z","shell.execute_reply":"2024-05-15T11:21:28.070357Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(r=4, target_modules='transformer\\.h\\..*\\.attn\\.c_.*',)\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T11:22:14.624315Z","iopub.execute_input":"2024-05-15T11:22:14.625049Z","iopub.status.idle":"2024-05-15T11:22:14.682650Z","shell.execute_reply.started":"2024-05-15T11:22:14.625015Z","shell.execute_reply":"2024-05-15T11:22:14.681759Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"trainable params: 221,184 || all params: 124,662,528 || trainable%: 0.17742621102629974\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"stanfordnlp/imdb\")\n\ndef tokenize(batch):\n    tokenized = tokenizer(batch[\"text\"], max_length=256, truncation=True, padding=True)\n    return {\"input_ids\": tokenized[\"input_ids\"], \"attention_mask\": tokenized[\"attention_mask\"]}\n\n\nds_train = dataset['train'].shuffle().select(range(10000))\nds_test = dataset['test'].shuffle().select(range(2500))\n\nds_train = ds_train.map(tokenize, batched=True)\nds_test = ds_test.map(tokenize, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T09:44:36.549413Z","iopub.execute_input":"2024-05-15T09:44:36.549821Z","iopub.status.idle":"2024-05-15T09:44:44.952857Z","shell.execute_reply.started":"2024-05-15T09:44:36.549787Z","shell.execute_reply":"2024-05-15T09:44:44.951832Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67c9493acfb6478d959d7c59becf595f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23671fc45c2e4254b27dd52fd7cc5c06"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom datasets import load_metric\nfrom transformers import TrainingArguments, Trainer\n\naccuracy_metric = load_metric(\"accuracy\")\nf1_metric = load_metric(\"f1\")\n\ndef compute_metrics(eval_pred):\n    predictions, label_ids = eval_pred.predictions, eval_pred.label_ids\n    predictions = predictions.argmax(axis=1)\n    accuracy = accuracy_metric.compute(predictions=predictions, references=label_ids)\n    f1 = f1_metric.compute(predictions=predictions, references=label_ids, average=\"weighted\")\n\n    return {\n        \"accuracy\": accuracy[\"accuracy\"],\n        \"f1\": f1[\"f1\"], \n    }","metadata":{"execution":{"iopub.status.busy":"2024-05-15T09:44:55.749495Z","iopub.execute_input":"2024-05-15T09:44:55.749923Z","iopub.status.idle":"2024-05-15T09:44:56.190460Z","shell.execute_reply.started":"2024-05-15T09:44:55.749890Z","shell.execute_reply":"2024-05-15T09:44:56.189303Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/f1/f1.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_arguments = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    num_train_epochs=10,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    learning_rate=1e-3,\n    weight_decay=1e-5,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"accuracy\",\n    report_to=\"none\"\n) \n\ntrainer = Trainer(\n    model=model,\n    train_dataset=ds_train,\n    eval_dataset=ds_test,\n    args=training_arguments,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:04:07.164199Z","iopub.execute_input":"2024-05-15T10:04:07.164594Z","iopub.status.idle":"2024-05-15T10:43:34.616013Z","shell.execute_reply.started":"2024-05-15T10:04:07.164559Z","shell.execute_reply":"2024-05-15T10:43:34.615143Z"},"trusted":true},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3130' max='3130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3130/3130 39:25, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.330400</td>\n      <td>0.248348</td>\n      <td>0.898000</td>\n      <td>0.897971</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.250800</td>\n      <td>0.225742</td>\n      <td>0.908800</td>\n      <td>0.908759</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.213700</td>\n      <td>0.220402</td>\n      <td>0.908000</td>\n      <td>0.907959</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.189400</td>\n      <td>0.232260</td>\n      <td>0.917600</td>\n      <td>0.917590</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.161500</td>\n      <td>0.228126</td>\n      <td>0.912000</td>\n      <td>0.911999</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.136300</td>\n      <td>0.276388</td>\n      <td>0.901600</td>\n      <td>0.901377</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.116200</td>\n      <td>0.272233</td>\n      <td>0.911200</td>\n      <td>0.911188</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.100800</td>\n      <td>0.273334</td>\n      <td>0.912400</td>\n      <td>0.912400</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.084200</td>\n      <td>0.302037</td>\n      <td>0.911200</td>\n      <td>0.911160</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.072300</td>\n      <td>0.311203</td>\n      <td>0.908000</td>\n      <td>0.907998</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3130, training_loss=0.16555457556971345, metrics={'train_runtime': 2366.4893, 'train_samples_per_second': 42.257, 'train_steps_per_second': 1.323, 'total_flos': 1.3098811392e+16, 'train_loss': 0.16555457556971345, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
